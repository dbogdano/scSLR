{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "233d5029-cbb1-4517-a114-f46a1bbdceca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Dec 10 15:18:21 PST 2024\n",
      "c4-dev1\n"
     ]
    }
   ],
   "source": [
    "!date\n",
    "!hostname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ebabb05c-2b99-4a4d-9ced-dff033cd4c37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/c4/home/derek/miniconda3/envs/deepripe'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%env CONDA_PREFIX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bbebc8b3-b742-4c7b-809a-471a5b2366a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2f86db3-0a6c-4cc9-80c0-3e12612e8c5e",
   "metadata": {},
   "source": [
    "# Generation of DeepRipe ExplaiNN input data from scSLR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "377921bd-4248-4b81-8337-14e10937bfa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import glob\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "from sklearn.model_selection import GroupKFold, ShuffleSplit, LeavePOut, RepeatedKFold\n",
    "\n",
    "import pysam "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "56dfcd06-29c3-44e7-adff-34c4c99397cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bea17728-e712-49d8-a6cb-50c169989448",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import modified basenji modules \n",
    "sys.path.append(\"/nowakowskilab/data1/derek/data_scSLR/prenatal_brain/deep_splicing\")\n",
    "from basenji_.basenji.dna_io import dna_1hot\n",
    "\n",
    "def region_to_mat(region):\n",
    "    region_len = len(region)\n",
    "    region= region.replace('i','0')\n",
    "    region= region.replace('c','1')\n",
    "    region= region.replace('3','2')\n",
    "    region= region.replace('5','3')\n",
    "    region= region.replace('N','4')\n",
    "    region_code = np.zeros((4,region_len), dtype='float16')\n",
    "    for i in range(region_len):\n",
    "        if int(region[i]) != 4:\n",
    "            region_code[int(region[i]),i] = 1\n",
    "        else:\n",
    "            region_code[0:4,i] = np.tile(0,4)\n",
    "    return np.transpose(region_code).astype('int8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f86ca514-4746-465d-b572-421cb71b28a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8346b5c-a03c-4b72-a7d7-e176d5f6d185",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0780c1d-d619-498d-90ce-5722af340527",
   "metadata": {},
   "outputs": [],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe8691c2-8d25-4879-b412-9ed913d3d8ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a61caf19-ddf1-42c0-8f4d-701c9388c74f",
   "metadata": {},
   "outputs": [],
   "source": [
    "in_dir = '/nowakowskilab/data1/derek/data_scSLR/prenatal_brain/deep_splicing/transfer_learning/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d5ca5f0-002a-4496-bfe8-702a876b25c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "EN = pd.read_csv(in_dir+'sig_dif_EN_table_0.1_.csv', index_col=0)\n",
    "RG = pd.read_csv(in_dir+'sig_dif_RG_table_0.1_.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d38adbf-ee9a-46ea-9871-8b1b9947f637",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0742d4ee-d5c1-4067-ab25-33614650bcec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e015353e-262d-4525-ad75-6ce7de6b6bb4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# EN_cas = pd.read_csv(in_dir+'sig_dif_EN_table_0.1_.csv', index_col=0)\n",
    "# RG_cas = pd.read_csv(in_dir+'sig_dif_RG_table_0.1_.csv', index_col=0)\n",
    "\n",
    "# EN_mxe = pd.read_csv(in_dir+'sig_dif_EN_table_MXE_0.1_.csv', index_col=0)\n",
    "# RG_mxe = pd.read_csv(in_dir+'sig_dif_RG_table_MXE_0.1_.csv', index_col=0)\n",
    "\n",
    "# EN_tand = pd.read_csv(in_dir+'sig_dif_EN_table_tand_0.1_.csv', index_col=0)\n",
    "# RG_tand = pd.read_csv(in_dir+'sig_dif_RG_table_tand_0.1_.csv', index_col=0)\n",
    "\n",
    "# EN_AFE = pd.read_csv(in_dir+'sig_dif_EN_table_AFE_0.1_.csv', index_col=0)\n",
    "# RG_AFE = pd.read_csv(in_dir+'sig_dif_RG_table_AFE_0.1_.csv', index_col=0)\n",
    "\n",
    "# EN = pd.concat([EN_cas,EN_mxe,EN_tand, EN_AFE]).drop_duplicates(subset='model_input')\n",
    "\n",
    "# RG = pd.concat([RG_cas,RG_mxe,RG_tand, RG_AFE]).drop_duplicates(subset='model_input')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5fdf5b7-8c4e-4756-abf2-86386824c278",
   "metadata": {},
   "outputs": [],
   "source": [
    "EN = EN[['model_input','delta_psi']].set_index('model_input')\n",
    "\n",
    "# EN['delta_psi'] = EN['delta_psi']*-1\n",
    "\n",
    "EN.columns = [0]\n",
    "EN.index.name = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9203cf72-38c7-4c9a-8707-657b7829efb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "RG = RG[['model_input','delta_psi']].set_index('model_input')\n",
    "\n",
    "# RG['delta_psi'] = RG['delta_psi']*-1\n",
    "\n",
    "RG.columns = [0]\n",
    "RG.index.name = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80a44e7d-8cd8-424a-9722-7a42604d7036",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([EN,RG],axis=1)\n",
    "df.columns = [0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a688fe32-62ca-4d51-b8f4-71bb4f83c610",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a64f949f-16f8-41a9-9bbd-be14e25bb6fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "exon_info = pd.DataFrame(df.index)[0].str.split('_', expand=True)\n",
    "exon_info = exon_info.rename(columns={0: 'chr', 1: 'start', 2: 'end',\n",
    "                                      3: 'ENSG', 4: 'strand'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91d48646-eb8f-43b7-bbda-c63c429253b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.iloc[exon_info.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6293105a-4395-40fc-a410-d7ab955081de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32004b66-cca5-4093-97fe-822a72c0a9f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "intron_flanking_length = 200\n",
    "\n",
    "exon_info['start'] = exon_info['start'].astype(int) - intron_flanking_length\n",
    "exon_info['end'] = exon_info['end'].astype(int) + intron_flanking_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24f02025-cc04-4ebc-affb-2f4f38cbc0d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "exon_info_set = exon_info.copy()\n",
    "\n",
    "num_targets = 2\n",
    "max_length = 800\n",
    "region_length = 1000\n",
    "\n",
    "extra = int((region_length - max_length)/8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "909a281f-b4d7-4df5-b567-72451018f56b",
   "metadata": {},
   "outputs": [],
   "source": [
    "exon_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f02c526c-5e1b-4826-93e2-2a280d7678cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f8291f8-4145-451e-bdba-2182a8951761",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c85948fb-de35-4b7a-91a9-fccefd01ef84",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "intron_flanking_length = 200\n",
    "exon_flanking_length = 200\n",
    "\n",
    "\n",
    "def create_tfrecords_deepripe(PSI, exon_info, train_idxs, val_idxs, \n",
    "                     test_idxs, fold, general_out_dir, peaks,\n",
    "                     max_length):\n",
    "    \n",
    "    fold_dir = general_out_dir + '/fold' + str(fold)\n",
    "    tfr_dir = fold_dir + '/tfrecords' \n",
    "    Path(fold_dir).mkdir(parents=True, exist_ok=True)\n",
    "    Path(tfr_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    fold_indexes = [train_idxs, val_idxs, test_idxs]\n",
    "    \n",
    "    # Write the file genes.csv\n",
    "    split = np.zeros((len(exon_info),1), dtype='<U5')\n",
    "    split[train_idxs] = 'train'\n",
    "    split[val_idxs] = 'valid'\n",
    "    split[test_idxs] = 'test'\n",
    "    target = PSI.values\n",
    "    genes = pd.DataFrame(np.hstack((split,target)), \n",
    "                         index=PSI.index, columns=np.hstack((['split'], PSI.columns)))\n",
    "    \n",
    "    genes = genes[genes['split'] != '']\n",
    "    genes.to_csv(fold_dir + '/genes.csv')\n",
    "    \n",
    "    # Options TFR writer\n",
    "    tf_opts = tf.io.TFRecordOptions(compression_type='ZLIB')\n",
    "    seqs_per_tfr = 256\n",
    "    fold_labels = ['train', 'valid', 'test']\n",
    "\n",
    "    \n",
    "    # open FASTA\n",
    "    fasta_open = pysam.Fastafile(file_fasta)\n",
    "    \n",
    "    def rc(seq):\n",
    "        return seq.translate(str.maketrans(\"ATCGatcg\",\"TAGCtagc\"))[::-1]\n",
    "    \n",
    "    def _bytes_feature(value):\n",
    "        return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n",
    "    \n",
    "    # Write the TFRecords\n",
    "    num_folds = 3 #train-val-test\n",
    "    \n",
    "    for fi in range(num_folds):\n",
    "        exon_ID_set = PSI.index[fold_indexes[fi]]\n",
    "        exon_info_set = exon_info.iloc[fold_indexes[fi]]\n",
    "        PSI_val_set = PSI.iloc[fold_indexes[fi]]\n",
    "        \n",
    "        num_set = exon_ID_set.shape[0]\n",
    "        \n",
    "        num_set_tfrs = int(np.ceil(num_set / seqs_per_tfr)) \n",
    "        # print(num_set_tfrs)\n",
    "        \n",
    "        # gene sequence index\n",
    "        si = 0\n",
    "        \n",
    "        for tfr_i in range(num_set_tfrs):\n",
    "            # Create the file e.g 'tfr_records/test-0.tfr'\n",
    "            tfr_file = '%s/%s-%d.tfr' % (tfr_dir, fold_labels[fi], tfr_i)\n",
    "            # print(tfr_file)\n",
    "            with tf.io.TFRecordWriter(tfr_file, tf_opts) as writer:\n",
    "                # TFR index\n",
    "                ti = 0\n",
    "                \n",
    "                # This is to make sure that the max. genes per file stays below 256\n",
    "                # And that for the last batch we stop on time (si < num_set)\n",
    "                while ti < seqs_per_tfr and si < num_set:\n",
    "                    # Get the genes that should be in this set\n",
    "                    \n",
    "                    seq_chrm = exon_info_set['chr'].iloc[si]\n",
    "                    seq_start = int(exon_info_set['start'].iloc[si])\n",
    "                    seq_end = int(exon_info_set['end'].iloc[si])+1 #Because fasta.fetch doesn't include end bp\n",
    "                    seq_strand = exon_info_set['strand'].iloc[si]\n",
    "                    gene = exon_info_set['ENSG'].iloc[si]\n",
    "                    \n",
    "                    #start\n",
    "                    seq_start_ = seq_start\n",
    "                    #end\n",
    "                    seq_end_ = seq_end\n",
    "                    \n",
    "                    \n",
    "                    #get exon's range\n",
    "                    seq_DNA = fasta_open.fetch(seq_chrm, seq_start_, seq_end_)\n",
    "                    \n",
    "                    #and its length\n",
    "                    len_DNA = len(seq_DNA)\n",
    "                    \n",
    "                    # get positions of splice sites\n",
    "                    splicing_ind = np.array([intron_flanking_length, max_length - intron_flanking_length], dtype=np.int64)\n",
    "\n",
    "                    \n",
    "                    # orient\n",
    "                    if seq_strand == '-':\n",
    "                        seq_DNA = rc(seq_DNA)\n",
    "                        \n",
    "                        \n",
    "                    # define ranges for all regions of interest and pad if shorter than intron_flanking_length\n",
    "                    \n",
    "                    \n",
    "                    # upstream_intron \n",
    "                    \n",
    "                    upstream_intron = seq_DNA[0:intron_flanking_length]\n",
    "                    \n",
    "                    upstream_intron_region = ('i'*extra)+('i'*intron_flanking_length)+('c'*extra)\n",
    "                    \n",
    "                    #exon_start\n",
    "                    \n",
    "                    exon_coord = len(seq_DNA) // 2\n",
    "                    \n",
    "                    if exon_coord <= (exon_flanking_length * 2):\n",
    "                    \n",
    "                        exon_start = seq_DNA[intron_flanking_length:exon_coord]\n",
    "                    \n",
    "                        padding_start = exon_flanking_length - len(exon_start)\n",
    "\n",
    "                        exon_start = exon_start + padding_start*'N'\n",
    "\n",
    "                        \n",
    "                    else: \n",
    "                        \n",
    "                        exon_start =  seq_DNA[intron_flanking_length:(intron_flanking_length + exon_flanking_length)]\n",
    "\n",
    "\n",
    "                    \n",
    "                    \n",
    "                    exon_start_region = ('i'*extra)+''.join(['c' if x != 'N' else 'N' for x in exon_start])+('N'*extra)\n",
    "                    \n",
    "                    # downstream_intron\n",
    "                    \n",
    "                    downstream_intron = seq_DNA[(len_DNA-intron_flanking_length):len_DNA]\n",
    "                    \n",
    "                    downstream_intron_region = ('c'*extra)+('i'*intron_flanking_length)+('i'*extra)\n",
    "\n",
    "                    # exon_end\n",
    "                    \n",
    "                    exon_coord = len(seq_DNA) // 2 \n",
    "                    \n",
    "                    if exon_coord <= (exon_flanking_length * 2): \n",
    "                    \n",
    "                        exon_end = seq_DNA[exon_coord:(len_DNA-intron_flanking_length)]\n",
    "                        \n",
    "                        padding_end = exon_flanking_length - len(exon_end)\n",
    "\n",
    "                        exon_end = padding_end*'N' + exon_end\n",
    "\n",
    "        \n",
    "                        \n",
    "                    else:\n",
    "                        \n",
    "                        exon_end = seq_DNA[(len_DNA-(intron_flanking_length + exon_flanking_length)):(len_DNA-intron_flanking_length)]\n",
    "                        \n",
    "                    exon_end_region = ('N'*extra)+''.join(['c' if x != 'N' else 'N' for x in exon_end])+('i'*extra)\n",
    "                        \n",
    "                        \n",
    "                    seq_DNA = (upstream_intron + exon_start + exon_end + downstream_intron)\n",
    "                      \n",
    "                    new_LEN = len(seq_DNA)\n",
    "\n",
    "                    \n",
    "                    assert(new_LEN == max_length)\n",
    "                    \n",
    "                    seq_region = upstream_intron_region + exon_start_region + exon_end_region + downstream_intron_region\n",
    "                    assert(len(seq_region) == region_length)\n",
    "                \n",
    "                  \n",
    "                    \n",
    "                    # one hot code\n",
    "                    seq_1hot = dna_1hot(seq_DNA)\n",
    "                    seq_len = np.array(len(seq_DNA), dtype=np.int64)\n",
    "                    \n",
    "                    # splicing\n",
    "                    splicing = np.zeros((new_LEN ,1), dtype=np.int8)\n",
    "                    splicing[splicing_ind] = 1\n",
    "                    \n",
    "                    # region \n",
    "                    region_1hot = region_to_mat(seq_region)\n",
    "           \n",
    "                    # get targets\n",
    "                    targets = PSI_val_set.iloc[si].values\n",
    "                    targets = targets.reshape((1,-1)).astype('float64')\n",
    "                    \n",
    "            ### generate example ###\n",
    "                    example = tf.train.Example(features=tf.train.Features(feature={\n",
    "                           \n",
    "                            'length': _bytes_feature(seq_len.flatten().tobytes()),\n",
    "                            'sequence': _bytes_feature(seq_1hot.flatten().tobytes()),\n",
    "                            'target': _bytes_feature(targets.flatten().tobytes()),\n",
    "                            'splicing': _bytes_feature(splicing.flatten().tobytes()), \n",
    "                            'region': _bytes_feature(region_1hot.flatten().tobytes())\n",
    "                        }))\n",
    "    \n",
    "                    # write\n",
    "                    writer.write(example.SerializeToString())\n",
    "    \n",
    "                    # advance indexes\n",
    "                    ti += 1\n",
    "                    si += 1\n",
    "    \n",
    "    \n",
    "    fasta_open.close()\n",
    "    \n",
    "    \n",
    "    # Write statistics.json\n",
    "    \n",
    "    stats_dict = {}\n",
    "    stats_dict['num_targets'] = num_targets\n",
    "    stats_dict['seq_length'] = max_length\n",
    "    stats_dict['target_length'] = 4\n",
    "    \n",
    "    for fi in range(num_folds):\n",
    "        stats_dict['%s_seqs' % fold_labels[fi]] = len(fold_indexes[fi])\n",
    "    \n",
    "    with open('%s/statistics.json' % fold_dir, 'w') as stats_json_out:\n",
    "        json.dump(stats_dict, stats_json_out, indent=4)\n",
    "    \n",
    "    \n",
    "    # Copy the params.json\n",
    "    train_dict = {}\n",
    "    train_dict['batch_size'] = 64\n",
    "    train_dict['optimizer'] = 'adam'\n",
    "    train_dict['loss'] = 'bce' #'mse'\n",
    "    train_dict['learning_rate'] = 0.0001\n",
    "    train_dict['adam_beta1'] = 0.90\n",
    "    train_dict['adam_beta2'] = 0.998\n",
    "    train_dict['global_clipnorm'] = 0.5\n",
    "    train_dict['train_epochs_min'] = 100\n",
    "    train_dict['train_epochs_max'] = 1000\n",
    "    train_dict['patience'] = 100\n",
    "    \n",
    "    model_dict = {}\n",
    "    model_dict['activation'] = 'relu'\n",
    "    model_dict['spline'] = False\n",
    "    model_dict['rnn_type'] = 'gru'\n",
    "    model_dict['final_activation'] = 'relu'\n",
    "    model_dict['residual'] = False\n",
    "    model_dict['seq_length'] = max_length\n",
    "\n",
    "    model_dict['seq_depth'] = 4 \n",
    "    model_dict['augment_shift'] = 0\n",
    "    model_dict['num_targets'] = num_targets\n",
    "    model_dict['heads'] = 1\n",
    "    model_dict['filters'] = 186\n",
    "    model_dict['kernel_size'] = 6 #5\n",
    "    model_dict['dropout'] = 0.3 \n",
    "    model_dict['l2_scale'] = 0.001\n",
    "    model_dict['ln_epsilon'] = 0.007\n",
    "    model_dict['bn_momentum'] = 0.90\n",
    "    \n",
    "    params_dict = {}\n",
    "    params_dict['train'] = train_dict\n",
    "    params_dict['model'] = model_dict\n",
    "    \n",
    "    with open('%s/params.json' % fold_dir, 'w') as params_json_out:\n",
    "        json.dump(params_dict, params_json_out, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1c7bbc3-13f9-46e3-a728-5f7d6ef88fac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tfrecords_deepripe(PSI, exon_info, train_idxs, val_idxs, \n",
    "                     test_idxs, fold, general_out_dir, peaks,\n",
    "                     max_length):\n",
    "    \n",
    "    fold_dir = general_out_dir + '/fold' + str(fold)\n",
    "    tfr_dir = fold_dir + '/tfrecords' \n",
    "    Path(fold_dir).mkdir(parents=True, exist_ok=True)\n",
    "    Path(tfr_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    fold_indexes = [train_idxs, val_idxs, test_idxs]\n",
    "    \n",
    "    # Write the file genes.csv\n",
    "    split = np.zeros((len(exon_info),1), dtype='<U5')\n",
    "    split[train_idxs] = 'train'\n",
    "    split[val_idxs] = 'valid'\n",
    "    split[test_idxs] = 'test'\n",
    "    target = PSI.values\n",
    "    genes = pd.DataFrame(np.hstack((split,target)), \n",
    "                         index=PSI.index, columns=np.hstack((['split'], PSI.columns)))\n",
    "    \n",
    "    genes = genes[genes['split'] != '']\n",
    "    genes.to_csv(fold_dir + '/genes.csv')\n",
    "    \n",
    "    # Options TFR writer\n",
    "    tf_opts = tf.io.TFRecordOptions(compression_type='ZLIB')\n",
    "    seqs_per_tfr = 256\n",
    "    fold_labels = ['train', 'valid', 'test']\n",
    "\n",
    "    \n",
    "    # open FASTA\n",
    "    fasta_open = pysam.Fastafile(file_fasta)\n",
    "    \n",
    "    def rc(seq):\n",
    "        return seq.translate(str.maketrans(\"ATCGatcg\",\"TAGCtagc\"))[::-1]\n",
    "    \n",
    "    def _bytes_feature(value):\n",
    "        return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n",
    "    \n",
    "    # Write the TFRecords\n",
    "    num_folds = 3 #train-val-test\n",
    "    \n",
    "    for fi in range(num_folds):\n",
    "        exon_ID_set = PSI.index[fold_indexes[fi]]\n",
    "        exon_info_set = exon_info.iloc[fold_indexes[fi]]\n",
    "        PSI_val_set = PSI.iloc[fold_indexes[fi]]\n",
    "        \n",
    "        num_set = exon_ID_set.shape[0]\n",
    "        \n",
    "        num_set_tfrs = int(np.ceil(num_set / seqs_per_tfr)) \n",
    "        # print(num_set_tfrs)\n",
    "        \n",
    "        # gene sequence index\n",
    "        si = 0\n",
    "        \n",
    "        for tfr_i in range(num_set_tfrs):\n",
    "            # Create the file e.g 'tfr_records/test-0.tfr'\n",
    "            tfr_file = '%s/%s-%d.tfr' % (tfr_dir, fold_labels[fi], tfr_i)\n",
    "            # print(tfr_file)\n",
    "            with tf.io.TFRecordWriter(tfr_file, tf_opts) as writer:\n",
    "                # TFR index\n",
    "                ti = 0\n",
    "                \n",
    "                # This is to make sure that the max. genes per file stays below 256\n",
    "                # And that for the last batch we stop on time (si < num_set)\n",
    "                while ti < seqs_per_tfr and si < num_set:\n",
    "                    # Get the genes that should be in this set\n",
    "                    \n",
    "                    seq_chrm = exon_info_set['chr'].iloc[si]\n",
    "                    seq_start = int(exon_info_set['start'].iloc[si])\n",
    "                    seq_end = int(exon_info_set['end'].iloc[si])+1 #Because fasta.fetch doesn't include end bp\n",
    "                    seq_strand = exon_info_set['strand'].iloc[si]\n",
    "                    gene = exon_info_set['ENSG'].iloc[si]\n",
    "                    \n",
    "                    #start\n",
    "                    seq_start_ = seq_start\n",
    "                    #end\n",
    "                    seq_end_ = seq_end\n",
    "                    \n",
    "                    \n",
    "                    #get exon's range\n",
    "                    seq_DNA = fasta_open.fetch(seq_chrm, seq_start_, seq_end_)\n",
    "                    \n",
    "                    #and its length\n",
    "                    len_DNA = len(seq_DNA)\n",
    "                    \n",
    "                    # get positions of splice sites\n",
    "                    splicing_ind = np.array([intron_flanking_length, max_length - intron_flanking_length], dtype=np.int64)\n",
    "\n",
    "                    \n",
    "                    # orient\n",
    "                    if seq_strand == '-':\n",
    "                        seq_DNA = rc(seq_DNA)\n",
    "                        \n",
    "                        \n",
    "                    # define ranges for all regions of interest and pad if shorter than intron_flanking_length\n",
    "                    \n",
    "                    \n",
    "                    # upstream_intron \n",
    "                    \n",
    "                    upstream_intron = seq_DNA[0:intron_flanking_length]\n",
    "                    \n",
    "                    upstream_intron_region = ('i'*extra)+('i'*intron_flanking_length)+('c'*extra)\n",
    "                    \n",
    "                    #exon_start\n",
    "                    \n",
    "                    exon_coord = len(seq_DNA) // 2\n",
    "                    \n",
    "                    if exon_coord <= (intron_flanking_length * 2):\n",
    "                    \n",
    "                        exon_start = seq_DNA[intron_flanking_length:exon_coord]\n",
    "                    \n",
    "                        padding_start = intron_flanking_length - len(exon_start)\n",
    "\n",
    "                        exon_start = exon_start + padding_start*'N'\n",
    "\n",
    "                        \n",
    "                    else: \n",
    "                        \n",
    "                        exon_start =  seq_DNA[intron_flanking_length:(intron_flanking_length * 2)]\n",
    "\n",
    "                    \n",
    "                    \n",
    "                    exon_start_region = ('i'*extra)+''.join(['c' if x != 'N' else 'N' for x in exon_start])+('N'*extra)\n",
    "                    \n",
    "                    # downstream_intron\n",
    "                    \n",
    "                    downstream_intron = seq_DNA[(len_DNA-intron_flanking_length):len_DNA]\n",
    "                    \n",
    "                    downstream_intron_region = ('c'*extra)+('i'*intron_flanking_length)+('i'*extra)\n",
    "\n",
    "                    # exon_end\n",
    "                    \n",
    "                    exon_coord = len(seq_DNA) // 2 \n",
    "                \n",
    "                    if exon_coord <= (intron_flanking_length * 2):\n",
    "                    \n",
    "                        exon_end = seq_DNA[exon_coord:(len_DNA-intron_flanking_length)]\n",
    "                        \n",
    "                        padding_end = intron_flanking_length - len(exon_end)\n",
    "\n",
    "                        exon_end = padding_end*'N' + exon_end\n",
    "\n",
    "        \n",
    "                        \n",
    "                    else:\n",
    "                        \n",
    "                        exon_end = seq_DNA[(len_DNA-(intron_flanking_length * 2)):(len_DNA-intron_flanking_length)]\n",
    "\n",
    "                    exon_end_region = ('N'*extra)+''.join(['c' if x != 'N' else 'N' for x in exon_end])+('i'*extra)\n",
    "\n",
    "                    \n",
    "                        \n",
    "                    seq_DNA = (upstream_intron + exon_start + exon_end + downstream_intron)\n",
    "                      \n",
    "                    new_LEN = len(seq_DNA)\n",
    "                    assert(new_LEN == max_length)\n",
    "                    \n",
    "                    seq_region = upstream_intron_region + exon_start_region + exon_end_region + downstream_intron_region\n",
    "                    assert(len(seq_region) == region_length)\n",
    "                \n",
    "                  \n",
    "                    \n",
    "                    # one hot code\n",
    "                    seq_1hot = dna_1hot(seq_DNA)\n",
    "                    seq_len = np.array(len(seq_DNA), dtype=np.int64)\n",
    "                    \n",
    "                    # splicing\n",
    "                    splicing = np.zeros((new_LEN ,1), dtype=np.int8)\n",
    "                    splicing[splicing_ind] = 1\n",
    "                    \n",
    "                    # region \n",
    "                    region_1hot = region_to_mat(seq_region)\n",
    "           \n",
    "                    # get targets\n",
    "                    targets = PSI_val_set.iloc[si].values\n",
    "                    targets = targets.reshape((1,-1)).astype('float64')\n",
    "                    \n",
    "            ### generate example ###\n",
    "                    example = tf.train.Example(features=tf.train.Features(feature={\n",
    "                           \n",
    "                            'length': _bytes_feature(seq_len.flatten().tobytes()),\n",
    "                            'sequence': _bytes_feature(seq_1hot.flatten().tobytes()),\n",
    "                            'target': _bytes_feature(targets.flatten().tobytes()),\n",
    "                            'splicing': _bytes_feature(splicing.flatten().tobytes()), \n",
    "                            'region': _bytes_feature(region_1hot.flatten().tobytes())\n",
    "                        }))\n",
    "    \n",
    "                    # write\n",
    "                    writer.write(example.SerializeToString())\n",
    "    \n",
    "                    # advance indexes\n",
    "                    ti += 1\n",
    "                    si += 1\n",
    "    \n",
    "    \n",
    "    fasta_open.close()\n",
    "    \n",
    "    \n",
    "    # Write statistics.json\n",
    "    \n",
    "    stats_dict = {}\n",
    "    stats_dict['num_targets'] = num_targets\n",
    "    stats_dict['seq_length'] = max_length\n",
    "    stats_dict['target_length'] = 4\n",
    "    \n",
    "    for fi in range(num_folds):\n",
    "        stats_dict['%s_seqs' % fold_labels[fi]] = len(fold_indexes[fi])\n",
    "    \n",
    "    with open('%s/statistics.json' % fold_dir, 'w') as stats_json_out:\n",
    "        json.dump(stats_dict, stats_json_out, indent=4)\n",
    "    \n",
    "    \n",
    "    # Copy the params.json\n",
    "    train_dict = {}\n",
    "    train_dict['batch_size'] = 64\n",
    "    train_dict['optimizer'] = 'adam'\n",
    "    train_dict['loss'] = 'bce' #'mse'\n",
    "    train_dict['learning_rate'] = 0.0001\n",
    "    train_dict['adam_beta1'] = 0.90\n",
    "    train_dict['adam_beta2'] = 0.998\n",
    "    train_dict['global_clipnorm'] = 0.5\n",
    "    train_dict['train_epochs_min'] = 100\n",
    "    train_dict['train_epochs_max'] = 1000\n",
    "    train_dict['patience'] = 100\n",
    "    \n",
    "    model_dict = {}\n",
    "    model_dict['activation'] = 'relu'\n",
    "    model_dict['spline'] = False\n",
    "    model_dict['rnn_type'] = 'gru'\n",
    "    model_dict['final_activation'] = 'relu'\n",
    "    model_dict['residual'] = False\n",
    "    model_dict['seq_length'] = max_length\n",
    "\n",
    "    model_dict['seq_depth'] = 4 \n",
    "    model_dict['augment_shift'] = 0\n",
    "    model_dict['num_targets'] = num_targets\n",
    "    model_dict['heads'] = 1\n",
    "    model_dict['filters'] = 186\n",
    "    model_dict['kernel_size'] = 6 #5\n",
    "    model_dict['dropout'] = 0.3 \n",
    "    model_dict['l2_scale'] = 0.001\n",
    "    model_dict['ln_epsilon'] = 0.007\n",
    "    model_dict['bn_momentum'] = 0.90\n",
    "    \n",
    "    params_dict = {}\n",
    "    params_dict['train'] = train_dict\n",
    "    params_dict['model'] = model_dict\n",
    "    \n",
    "    with open('%s/params.json' % fold_dir, 'w') as params_json_out:\n",
    "        json.dump(params_dict, params_json_out, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d919f3dc-127f-454d-b62c-364ca56116ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "genes = exon_info['ENSG']\n",
    "num_fold = 10\n",
    "file_folds = None\n",
    "peaks=0\n",
    "\n",
    "\n",
    "file_fasta = \"/c4/home/derek/data1/derek/reference/human_hg38_reference/refdata-gex-GRCh38-2020-A/fasta/genome.fa\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73440529-62f9-4b5c-ac57-31eb1643e87b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27ae8f34-5969-438f-b9ec-ac99b0afee6f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "08ad873e-f570-4647-9069-acdc93541173",
   "metadata": {},
   "source": [
    "## Kfold Cross validation data setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "49b10a84-b855-4827-a459-3037e31c32de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# out_dir = 'data_out/EN_RG_multitask_DeltaPSI_sig_0.1_train_valid_deepripe_PARCLIP/'\n",
    "\n",
    "# out_dir= 'data_out/EN_RG_multitask_DeltaPSI_sig_0.1_train_valid_deepripe_sho/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4d41df6-d218-4e64-b8ac-152335c5c535",
   "metadata": {},
   "source": [
    "### train, valid, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "781e63f5-ba8e-448f-ae1d-4f9b08f4ff07",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "# # split into train valid and test\n",
    "# cv = GroupKFold(n_splits = num_fold)\n",
    "\n",
    "# for i in range(num_fold):\n",
    "\n",
    "        \n",
    "#     train_test_indices = list(cv.split(df, df, genes))\n",
    "#     train_val_idxs, test_idxs = train_test_indices[i]\n",
    "    \n",
    "#     cv2 = GroupKFold(n_splits = 5)\n",
    "    \n",
    "#     train_val_indices = list(cv2.split(df.iloc[train_val_idxs],\n",
    "#                                        df.iloc[train_val_idxs],\n",
    "#                                        genes[train_val_idxs]))\n",
    "    \n",
    "#     train_idxs, val_idxs = train_val_indices[0]\n",
    "#     train_idxs = train_val_idxs[train_idxs]\n",
    "#     val_idxs = train_val_idxs[val_idxs]\n",
    "    \n",
    "\n",
    "#     create_tfrecords_deepripe(df, exon_info, train_idxs,\n",
    "#                      val_idxs, test_idxs, i, \n",
    "#                      out_dir, peaks,\n",
    "#                      max_length)\n",
    "\n",
    "\n",
    "#     num_fold += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "667811a4-4e0f-4545-9194-35fb70e4bfa1",
   "metadata": {},
   "source": [
    "### train, valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8db0a4e5-1340-42de-aa0d-4074ceeac92a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8.1 s, sys: 69.6 ms, total: 8.17 s\n",
      "Wall time: 8.69 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# split into train and test only\n",
    "num_fold = 5\n",
    "\n",
    "cv = GroupKFold(n_splits = num_fold)\n",
    "\n",
    "## create data for single fold for testing\n",
    "for i in range(num_fold):\n",
    "\n",
    "    train_test_indices = list(cv.split(df, df, genes))\n",
    "    train_idxs, val_idxs = train_test_indices[i]\n",
    "    test_idxs= np.array([],dtype=int)\n",
    "    \n",
    "    val_idxs_ =np.concatenate([test_idxs,val_idxs]) #mixed val and test data\n",
    "\n",
    "    # create_tfrecords_deepripe(df, exon_info, train_idxs,\n",
    "    #                              val_idxs, test_idxs, i, \n",
    "    #                              out_dir, peaks,\n",
    "    #                              max_length)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd0b000e-d39b-4298-97f1-be63f63b4951",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "948cdfd9-37ca-49c8-88f1-d33843fc1b94",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5f9ce2b2-ec08-4798-a861-dc84f3fe88fb",
   "metadata": {},
   "source": [
    "## Repeated KFold Cross validation data setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "a5578e4a-44fb-4171-9f75-57b59906c516",
   "metadata": {},
   "outputs": [],
   "source": [
    "# out_dir = 'data_out/EN_RG_multitask_DeltaPSI_sig_0.1_train_valid_test_deepripe_repeatKfold/'\n",
    "\n",
    "out_dir = 'data_out/EN_RG_multitask_DeltaPSI_sig_0.1_train_valid_deepripe_/'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4c3edfca-8453-48db-9828-9d9f24f689d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/nowakowskilab/data1/derek/data_scSLR/prenatal_brain/deep_splicing/ExplaiNN'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "04d46f26-63ad-4634-9438-33964275d104",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "# rkf = RepeatedKFold(n_splits=10, n_repeats=10, random_state=2652124)\n",
    "\n",
    "# repeat = 0\n",
    "\n",
    "# out_dir_ = out_dir+f'/repeat_{repeat}/'\n",
    "\n",
    "\n",
    "# for i, (train_val_idxx, test_idxs) in enumerate(rkf.split(df)):\n",
    "        \n",
    "    \n",
    "#     cv2 = GroupKFold(n_splits = 5)\n",
    "    \n",
    "#     train_val_indices = list(cv2.split(df.iloc[train_val_idxx],\n",
    "#                                        df.iloc[train_val_idxx],\n",
    "#                                        genes[train_val_idxx]))\n",
    "    \n",
    "#     train_idxs, val_idxs = train_val_indices[0] #- take just first fold split \n",
    "#     train_idxs = train_val_idxs[train_idxs]\n",
    "#     val_idxs = train_val_idxs[val_idxs]\n",
    "    \n",
    "    \n",
    "    \n",
    "#     create_tfrecords_deepripe(df, exon_info, train_idxs,\n",
    "#                      val_idxs, test_idxs, i, \n",
    "#                      out_dir_, peaks,\n",
    "#                      max_length)\n",
    "    \n",
    "    \n",
    "#     if (i+1) % 10 == 0:\n",
    "        \n",
    "#         repeat +=1\n",
    "        \n",
    "#         os.mkdir(out_dir+f'/repeat_{repeat}/')\n",
    "        \n",
    "#         out_dir_ = out_dir+f'/repeat_{repeat}/'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6797e5dd-5093-42bc-b65e-9e91882e2f6d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "856a0d05-abf5-4d99-b40e-b38a9938270a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2e6c532-b7b6-49d4-84dd-ac8bb0cf7f01",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1099160-0955-4216-b6fd-b33a5afea637",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "1a26177c-5509-4636-96e7-6071a4fc1048",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 6s, sys: 513 ms, total: 2min 6s\n",
      "Wall time: 2min 7s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "rkf = RepeatedKFold(n_splits=10, n_repeats=5, random_state=2652124)\n",
    "\n",
    "repeat = 0\n",
    "\n",
    "out_dir_ = out_dir+f'/repeat_{repeat}/'\n",
    "\n",
    "\n",
    "for i, (train_idxx, val_idxs) in enumerate(rkf.split(df)):\n",
    "                \n",
    "    create_tfrecords_deepripe(df, exon_info, train_idxs,\n",
    "                     val_idxs, test_idxs, i, \n",
    "                     out_dir_, peaks,\n",
    "                     max_length)\n",
    "    \n",
    "    \n",
    "    if (i+1) % 10 == 0:\n",
    "        \n",
    "        repeat +=1\n",
    "        \n",
    "        os.mkdir(out_dir+f'/repeat_{repeat}/')\n",
    "        \n",
    "        out_dir_ = out_dir+f'/repeat_{repeat}/'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94e436fd-4bb3-4990-a19c-c69a2876b6a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f046b378-39d7-4657-b189-936c2b4fec1c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a44ebf3f-c37e-4964-beb7-e693e7086bf3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d17859ee-815c-48ff-b8f8-d33bc0005a04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# # split into train and test only\n",
    "# num_fold = 5\n",
    "\n",
    "# cv = GroupKFold(n_splits = num_fold)\n",
    "\n",
    "# ## create data for single fold for testing\n",
    "# for i in range(num_fold):\n",
    "\n",
    "#     train_test_indices = list(cv.split(df, df, genes))\n",
    "#     train_idxs, val_idxs = train_test_indices[i]\n",
    "#     test_idxs= np.array([],dtype=int)\n",
    "    \n",
    "#     val_idxs_ =np.concatenate([test_idxs,val_idxs]) #mixed val and test data\n",
    "\n",
    "#     create_tfrecords_deepripe(df, exon_info, train_idxs,\n",
    "#                                  val_idxs, test_idxs, i, \n",
    "#                                  out_dir, peaks,\n",
    "#                                  max_length)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ed40a06-93e7-4dfb-abd5-534230546e6a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e2e0493-7866-4eee-a7e0-775903e068ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "496a5e96-62ed-469d-8151-0ebe60860801",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0868a7d-b340-4748-bdf2-35f407cbc213",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a44f2d1c-9612-4a18-bfce-2fb2e81ae96b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9a508a3-fe71-422f-a028-9e622150dd16",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c586dc72-af52-44cc-9169-c706b7823b82",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75c369b5-2628-4a27-9414-070d6d3bd71d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "008898e8-73eb-4600-a8aa-865486a03a21",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa507e43-08cf-492f-b648-cd2d6cce69c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b8e8d74-d663-4cd3-a145-8cf77407164f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7ed1e691-9dfa-4efb-80f7-bb0cf845396c",
   "metadata": {},
   "source": [
    "### check tfrecord loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "72c43232-3e7f-439d-b0ed-febc8ace37bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "45f032a5-81a4-45df-bb0c-c22825674c15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "from basenji_.basenji import dataset_\n",
    "\n",
    "from basenji_.basenji.metrics import PearsonR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "85da96ab-33d8-4ef0-9e6b-bee979955957",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'data_out/EN_RG_multitask_DeltaPSI_sig_0.1_train_valid_deepripe_region/'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1fd9b8a4-ed81-43b9-ba4a-3012feecd204",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = [glob.glob(out_dir+'/fold*/')[0]] \n",
    "params_file = data_dir[0]+'params.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2dfe1582-003a-41cc-8be7-dd015b4c4997",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['data_out/EN_RG_multitask_DeltaPSI_sig_0.1_train_valid_deepripe_region/fold0/']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f1205b7d-42ff-4b04-ba91-80d40e25f388",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'data_out/EN_RG_multitask_DeltaPSI_sig_0.1_train_valid_deepripe_region/fold0/params.json'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "53e98c34-92e2-4ebb-9e83-1c5231c55722",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(params_file) as params_open:\n",
    "          params = json.load(params_open)\n",
    "params_model = params['model']\n",
    "params_train = params['train']\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "if params_file != '%s/params.json' % out_dir:\n",
    "    shutil.copy(params_file, '%s/params.json' % out_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46715e80-16d2-4d08-a238-f42ff006d81a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "0c9a493d-06b9-4b81-adda-c25f4cfb7224",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "for data_dir_ in data_dir:\n",
    "    train_data.append(dataset_.ExonDataset(data_dir_,\n",
    "        split_label='train',\n",
    "        batch_size=params_train['batch_size'],\n",
    "        shuffle_buffer=params_train.get('shuffle_buffer', params_model['seq_length']),\n",
    "        mode='train',\n",
    "        splice_track=False,\n",
    "        annotation_tracks=True\n",
    "                            ))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81fca8d8-18c9-4890-bef1-a310cfaf0d8e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8213adda-43a0-41f4-b712-aa04ad472930",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73f8e1ed-b132-4ef4-933f-78a7ad50e97b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
